{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('./src')\n",
    "from pipeline import GCLPipeline\n",
    "\n",
    "import os.path as osp\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch_geometric.transforms as T\n",
    "\n",
    "from torch.optim import Adam\n",
    "\n",
    "from tqdm import tqdm \n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "DATA_PATH = \"./data\"\n",
    "BENCHMARK_PATH = \"./benchmark\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "STRATEGIES = json.load(open(osp.join(BENCHMARK_PATH, \"benchmark.json\")))\n",
    "CONFIG = STRATEGIES[\"config\"]\n",
    "TRY_GPU = CONFIG[\"try_gpu\"]\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() and TRY_GPU else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "##### InfoGraph #####\n",
      "Dataset initialization\n",
      "\t # features: 18\n",
      "Encoder initialization\n",
      "\t input dim: 18\n",
      "\t hidden dim: 32\n",
      "\t # layers: 2\n",
      "\t projection dim: None\n",
      "\t activation: <class 'torch.nn.modules.activation.ReLU'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:10<00:00,  9.21it/s]it/s, loss=-2.24]\n",
      "(T): 100%|██████████| 100/100 [00:10<00:00,  9.20it/s, loss=-2.24]\n",
      "c:\\Users\\alexa\\anaconda3\\envs\\gcl-py311-cu121\\Lib\\site-packages\\sklearn\\svm\\_base.py:1237: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\alexa\\anaconda3\\envs\\gcl-py311-cu121\\Lib\\site-packages\\sklearn\\svm\\_base.py:1237: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\alexa\\anaconda3\\envs\\gcl-py311-cu121\\Lib\\site-packages\\sklearn\\svm\\_base.py:1237: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\alexa\\anaconda3\\envs\\gcl-py311-cu121\\Lib\\site-packages\\sklearn\\svm\\_base.py:1237: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\alexa\\anaconda3\\envs\\gcl-py311-cu121\\Lib\\site-packages\\sklearn\\svm\\_base.py:1237: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\alexa\\anaconda3\\envs\\gcl-py311-cu121\\Lib\\site-packages\\sklearn\\svm\\_base.py:1237: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(E): Best test F1Mi=0.5714, F1Ma=0.5478\n",
      "\n",
      "\n",
      "##### TransductiveDGI #####\n",
      "Dataset initialization\n",
      "\t # features: 1433\n",
      "Encoder initialization\n",
      "\t input dim: 1433\n",
      "\t hidden dim: 512\n",
      "\t # layers: 2\n",
      "\t projection dim: None\n",
      "\t activation: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 300/300 [00:11<00:00, 25.42it/s]it/s, loss=-1.36]  \n",
      "(T): 100%|██████████| 300/300 [00:11<00:00, 25.41it/s, loss=-1.36]\n",
      "(LR): 100%|██████████| 5000/5000 [00:15<00:00, best test F1Mi=0.82, F1Ma=0.772] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(E): Best test F1Mi=0.8199, F1Ma=0.7723\n",
      "\n",
      "\n",
      "##### GRACE #####\n",
      "Dataset initialization\n",
      "\t # features: 1433\n",
      "Encoder initialization\n",
      "\t input dim: 1433\n",
      "\t hidden dim: 32\n",
      "\t # layers: 2\n",
      "\t projection dim: 32\n",
      "\t activation: <class 'torch.nn.modules.activation.ReLU'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(T):   0%|          | 0/1000 [00:00<?, ?it/s]c:\\Users\\alexa\\anaconda3\\envs\\gcl-py311-cu121\\Lib\\site-packages\\torch_geometric\\deprecation.py:26: UserWarning: 'dropout_adj' is deprecated, use 'dropout_edge' instead\n",
      "  warnings.warn(out)\n",
      "100%|██████████| 1000/1000 [00:44<00:00, 22.49it/s]t/s, loss=3.93]\n",
      "(T): 100%|██████████| 1000/1000 [00:44<00:00, 22.49it/s, loss=3.93]\n",
      "(LR): 100%|██████████| 5000/5000 [00:14<00:00, best test F1Mi=0.79, F1Ma=0.773] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(E): Best test F1Mi=0.7904, F1Ma=0.7726\n",
      "\n",
      "\n",
      "##### GRACE #####\n",
      "Dataset initialization\n",
      "\t # features: 1433\n",
      "Encoder initialization\n",
      "\t input dim: 1433\n",
      "\t hidden dim: 32\n",
      "\t # layers: 2\n",
      "\t projection dim: 32\n",
      "\t activation: <class 'torch.nn.modules.activation.ReLU'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(T):   0%|          | 0/1000 [00:00<?, ?it/s]c:\\Users\\alexa\\anaconda3\\envs\\gcl-py311-cu121\\Lib\\site-packages\\torch_geometric\\deprecation.py:26: UserWarning: 'dropout_adj' is deprecated, use 'dropout_edge' instead\n",
      "  warnings.warn(out)\n",
      "  0%|          | 0/1000 [00:00<?, ?it/s]\n",
      "(T):   0%|          | 0/1000 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 30\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tqdm(total\u001b[38;5;241m=\u001b[39mepochs, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m(T)\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m pbar:\n\u001b[0;32m     29\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, epochs\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m)):\n\u001b[1;32m---> 30\u001b[0m         loss \u001b[38;5;241m=\u001b[39m \u001b[43mpipeline\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     31\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoder_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mDEVICE\u001b[49m\n\u001b[0;32m     32\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     33\u001b[0m         pbar\u001b[38;5;241m.\u001b[39mset_postfix({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m'\u001b[39m: loss})\n\u001b[0;32m     35\u001b[0m         pbar\u001b[38;5;241m.\u001b[39mupdate()\n",
      "File \u001b[1;32mc:\\Users\\alexa\\Documents\\Visual Studio Code\\graph-contrastive-learning\\./src\\pipeline.py:294\u001b[0m, in \u001b[0;36mGCLPipeline.train_epoch\u001b[1;34m(self, encoder_model, dataset, optimizer, device)\u001b[0m\n\u001b[0;32m    284\u001b[0m _, z1, z2 \u001b[38;5;241m=\u001b[39m encoder_model(\n\u001b[0;32m    285\u001b[0m     dataset\u001b[38;5;241m.\u001b[39mx\u001b[38;5;241m.\u001b[39mto(device),\n\u001b[0;32m    286\u001b[0m     dataset\u001b[38;5;241m.\u001b[39medge_index\u001b[38;5;241m.\u001b[39mto(device),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    291\u001b[0m     ),\n\u001b[0;32m    292\u001b[0m )\n\u001b[0;32m    293\u001b[0m h1, h2 \u001b[38;5;241m=\u001b[39m [encoder_model\u001b[38;5;241m.\u001b[39mproject(x) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m [z1, z2]]\n\u001b[1;32m--> 294\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontrast_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mh1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mh2\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    295\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m    296\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[1;32mc:\\Users\\alexa\\anaconda3\\envs\\gcl-py311-cu121\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\alexa\\anaconda3\\envs\\gcl-py311-cu121\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\alexa\\anaconda3\\envs\\gcl-py311-cu121\\Lib\\site-packages\\GCL\\models\\contrast_model.py:59\u001b[0m, in \u001b[0;36mDualBranchContrast.forward\u001b[1;34m(self, h1, h2, g1, g2, batch, h3, h4, extra_pos_mask, extra_neg_mask)\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# global-to-local\u001b[39;00m\n\u001b[0;32m     58\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m batch \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m batch\u001b[38;5;241m.\u001b[39mmax()\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m:  \u001b[38;5;66;03m# single graph\u001b[39;00m\n\u001b[1;32m---> 59\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mall\u001b[39m(v \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m [h1, h2, g1, g2, h3, h4])\n\u001b[0;32m     60\u001b[0m         anchor1, sample1, pos_mask1, neg_mask1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msampler(anchor\u001b[38;5;241m=\u001b[39mg1, sample\u001b[38;5;241m=\u001b[39mh2, neg_sample\u001b[38;5;241m=\u001b[39mh4)\n\u001b[0;32m     61\u001b[0m         anchor2, sample2, pos_mask2, neg_mask2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msampler(anchor\u001b[38;5;241m=\u001b[39mg2, sample\u001b[38;5;241m=\u001b[39mh1, neg_sample\u001b[38;5;241m=\u001b[39mh3)\n",
      "\u001b[1;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "outputs = []\n",
    "\n",
    "for strategy in STRATEGIES[\"strategies\"]:\n",
    "\n",
    "    print(\"\\n\")\n",
    "\n",
    "    # Initialization\n",
    "    pipeline = GCLPipeline.from_strategy(strategy, DEVICE)\n",
    "\n",
    "    # Data\n",
    "    dataset, num_features = GCLPipeline.init_dataset(\n",
    "        strategy[\"dataset\"], DATA_PATH, T.NormalizeFeatures(), strategy[\"batch_size\"]\n",
    "    )\n",
    "    \n",
    "    # Encoder\n",
    "    encoder_params = strategy[\"encoder_params\"]\n",
    "    encoder_params[\"input_dim\"] = num_features\n",
    "    encoder_model = pipeline.init_encoder(encoder_params, DEVICE)\n",
    "\n",
    "    # Training\n",
    "    train_params = strategy[\"train_params\"]\n",
    "    lr = train_params[\"learning_rate\"]\n",
    "    epochs = train_params[\"epochs\"]\n",
    "\n",
    "    optimizer = Adam(encoder_model.parameters(), lr=lr)\n",
    "    losses = []\n",
    "\n",
    "    with tqdm(total=epochs, desc='(T)') as pbar:\n",
    "        for epoch in tqdm(range(1, epochs+1)):\n",
    "            loss = pipeline.train_epoch(\n",
    "                encoder_model, dataset, optimizer, DEVICE\n",
    "            )\n",
    "            pbar.set_postfix({'loss': loss})\n",
    "\n",
    "            pbar.update()\n",
    "\n",
    "    test_result = pipeline.test(encoder_model, dataset, DEVICE)\n",
    "    print(f'(E): Best test F1Mi={test_result[\"micro_f1\"]:.4f}, F1Ma={test_result[\"macro_f1\"]:.4f}')\n",
    "\n",
    "    outputs.append({\n",
    "        \"Data\": strategy[\"dataset\"],\n",
    "        \"Method\": strategy[\"method\"],\n",
    "        \"Archi\": strategy[\"architecture\"],\n",
    "        \"Mode\": strategy[\"mode\"],\n",
    "        \"Obj\": strategy[\"objective\"],\n",
    "        \"Neg\": strategy[\"negative\"],\n",
    "\n",
    "        \"Aug1\": strategy[\"augmentation1\"],\n",
    "        \"Aug1Strat\": strategy[\"augmentation1_strat\"], \n",
    "        \"Aug2\": strategy[\"negative\"],\n",
    "        \"Aug2Strat\": strategy[\"negative\"], \n",
    "        \"MicroF1\": test_result[\"micro_f1\"],\n",
    "        \"MacroF1\": test_result[\"macro_f1\"]\n",
    "    })\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Data</th>\n",
       "      <th>Method</th>\n",
       "      <th>Archi</th>\n",
       "      <th>Mode</th>\n",
       "      <th>Obj</th>\n",
       "      <th>Neg</th>\n",
       "      <th>Aug1</th>\n",
       "      <th>Aug1Strat</th>\n",
       "      <th>Aug2</th>\n",
       "      <th>Aug2Strat</th>\n",
       "      <th>MicroF1</th>\n",
       "      <th>MacroF1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>PTC_MR</td>\n",
       "      <td>InfoGraph</td>\n",
       "      <td>SingleBranch</td>\n",
       "      <td>G2L</td>\n",
       "      <td>JSD</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.416667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Cora</td>\n",
       "      <td>TransductiveDGI</td>\n",
       "      <td>SingleBranch</td>\n",
       "      <td>G2L</td>\n",
       "      <td>JSD</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>Compose</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0.808824</td>\n",
       "      <td>0.780598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Cora</td>\n",
       "      <td>GRACE</td>\n",
       "      <td>DualBranch</td>\n",
       "      <td>L2L</td>\n",
       "      <td>InfoNCE</td>\n",
       "      <td>None</td>\n",
       "      <td>[EdgeRemoving, FeatureMasking]</td>\n",
       "      <td>Compose</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0.735294</td>\n",
       "      <td>0.714116</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Data           Method         Archi Mode      Obj   Neg  \\\n",
       "0  PTC_MR        InfoGraph  SingleBranch  G2L      JSD  None   \n",
       "1    Cora  TransductiveDGI  SingleBranch  G2L      JSD  None   \n",
       "2    Cora            GRACE    DualBranch  L2L  InfoNCE  None   \n",
       "\n",
       "                             Aug1 Aug1Strat  Aug2 Aug2Strat   MicroF1  \\\n",
       "0                            None      None  None      None  0.428571   \n",
       "1                            None   Compose  None      None  0.808824   \n",
       "2  [EdgeRemoving, FeatureMasking]   Compose  None      None  0.735294   \n",
       "\n",
       "    MacroF1  \n",
       "0  0.416667  \n",
       "1  0.780598  \n",
       "2  0.714116  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_df = pd.DataFrame(outputs)\n",
    "output_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gcl-py311-cu121",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
