{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "\n",
    "data_dir = '../data'\n",
    "dataset = ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Investigate example: Reddit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import os.path as osp\n",
    "from typing import Callable, List, Optional\n",
    "\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "import torch\n",
    "\n",
    "from torch_geometric.data import (\n",
    "    Data,\n",
    "    InMemoryDataset,\n",
    "    download_url,\n",
    "    extract_zip,\n",
    ")\n",
    "from torch_geometric.utils import coalesce\n",
    "\n",
    "\n",
    "class Reddit(InMemoryDataset):\n",
    "    r\"\"\"The Reddit dataset from the `\"Inductive Representation Learning on\n",
    "    Large Graphs\" <https://arxiv.org/abs/1706.02216>`_ paper, containing\n",
    "    Reddit posts belonging to different communities.\n",
    "\n",
    "    Args:\n",
    "        root (str): Root directory where the dataset should be saved.\n",
    "        transform (callable, optional): A function/transform that takes in an\n",
    "            :obj:`torch_geometric.data.Data` object and returns a transformed\n",
    "            version. The data object will be transformed before every access.\n",
    "            (default: :obj:`None`)\n",
    "        pre_transform (callable, optional): A function/transform that takes in\n",
    "            an :obj:`torch_geometric.data.Data` object and returns a\n",
    "            transformed version. The data object will be transformed before\n",
    "            being saved to disk. (default: :obj:`None`)\n",
    "        force_reload (bool, optional): Whether to re-process the dataset.\n",
    "            (default: :obj:`False`)\n",
    "\n",
    "    **STATS:**\n",
    "\n",
    "    .. list-table::\n",
    "        :widths: 10 10 10 10\n",
    "        :header-rows: 1\n",
    "\n",
    "        * - #nodes\n",
    "          - #edges\n",
    "          - #features\n",
    "          - #classes\n",
    "        * - 232,965\n",
    "          - 114,615,892\n",
    "          - 602\n",
    "          - 41\n",
    "    \"\"\"\n",
    "\n",
    "    url = 'https://data.dgl.ai/dataset/reddit.zip'\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        root: str,\n",
    "        transform: Optional[Callable] = None,\n",
    "        pre_transform: Optional[Callable] = None,\n",
    "        force_reload: bool = False,\n",
    "    ) -> None:\n",
    "        super().__init__(root, transform, pre_transform,\n",
    "                         force_reload=force_reload)\n",
    "        self.load(self.processed_paths[0])\n",
    "\n",
    "    @property\n",
    "    def raw_file_names(self) -> List[str]:\n",
    "        return ['reddit_data.npz', 'reddit_graph.npz']\n",
    "\n",
    "    @property\n",
    "    def processed_file_names(self) -> str:\n",
    "        return 'data.pt'\n",
    "\n",
    "    def download(self) -> None:\n",
    "        path = download_url(self.url, self.raw_dir)\n",
    "        extract_zip(path, self.raw_dir)\n",
    "        os.unlink(path)\n",
    "\n",
    "    def process(self) -> None:\n",
    "        data = np.load(osp.join(self.raw_dir, 'reddit_data.npz'))\n",
    "        x = torch.from_numpy(data['feature']).to(torch.float)\n",
    "        y = torch.from_numpy(data['label']).to(torch.long)\n",
    "        split = torch.from_numpy(data['node_types'])\n",
    "\n",
    "        adj = sp.load_npz(osp.join(self.raw_dir, 'reddit_graph.npz'))\n",
    "        row = torch.from_numpy(adj.row).to(torch.long)\n",
    "        col = torch.from_numpy(adj.col).to(torch.long)\n",
    "        edge_index = torch.stack([row, col], dim=0)\n",
    "        edge_index = coalesce(edge_index, num_nodes=x.size(0))\n",
    "\n",
    "        data = Data(x=x, edge_index=edge_index, y=y)\n",
    "        data.train_mask = split == 1\n",
    "        data.val_mask = split == 2\n",
    "        data.test_mask = split == 3\n",
    "\n",
    "        data = data if self.pre_transform is None else self.pre_transform(data)\n",
    "\n",
    "        self.save([data], self.processed_paths[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '../data'\n",
    "dataset_name = 'reddit'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.load(osp.join(data_dir, dataset_name, 'reddit_data.npz'))\n",
    "data # NpzFile '../data\\\\reddit\\\\reddit_data.npz' with keys: feature, node_types, node_ids, label\n",
    "type(data) # numpy.lib.npyio.NpzFile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.from_numpy(data['feature']).to(torch.float)\n",
    "x\n",
    "# tensor([[ 1.2334,  9.0430, -0.9233,  ..., -0.2579,  0.3112, -0.3772],\n",
    "#         [-0.1386, -0.2022,  0.1277,  ...,  0.1563,  0.1048, -0.6534],\n",
    "#         [-0.1330, -0.1962, -0.0296,  ...,  0.0358,  0.2864,  0.2744],\n",
    "#         ...,\n",
    "#         [-0.0614, -0.2022,  0.9698,  ...,  1.1064, -1.4323, -0.2398],\n",
    "#         [-0.1606, -0.2022, -0.0892,  ...,  0.7440, -0.5046, -2.2288],\n",
    "#         [ 0.0929,  0.2822,  0.1768,  ...,  0.2196,  0.5967,  0.5588]])\n",
    "x.shape\n",
    "# torch.Size([232965, 602])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = torch.from_numpy(data['label']).to(torch.long)\n",
    "y # tensor([30, 17, 18,  ...,  3, 13, 13])\n",
    "y.shape # torch.Size([232965])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split = torch.from_numpy(data['node_types'])\n",
    "split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adj = sp.load_npz(osp.join(data_dir, dataset_name, 'reddit_graph.npz'))\n",
    "adj\n",
    "# <232965x232965 sparse matrix of type '<class 'numpy.int64'>'\n",
    "# \twith 114615892 stored elements in COOrdinate format>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row = torch.from_numpy(adj.row).to(torch.long)\n",
    "col = torch.from_numpy(adj.col).to(torch.long)\n",
    "row # tensor([     0,      0,      0,  ..., 232920, 232931, 232952])\n",
    "col # tensor([225202, 177307, 107546,  ..., 232897, 232907, 232910])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_index = torch.stack([row, col], dim=0)\n",
    "edge_index\n",
    "# tensor([[     0,      0,      0,  ..., 232920, 232931, 232952],\n",
    "#         [225202, 177307, 107546,  ..., 232897, 232907, 232910]])\n",
    "edge_index.shape\n",
    "# torch.Size([2, 114615892])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_index = coalesce(edge_index, num_nodes=x.size(0))\n",
    "edge_index\n",
    "# tensor([[     0,      0,      0,  ..., 232964, 232964, 232964],\n",
    "#         [   242,    249,    524,  ..., 231806, 232594, 232634]])\n",
    "edge_index.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Node classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interaction Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path as osp\n",
    "import torch\n",
    "from torch_geometric.data import InMemoryDataset, Data\n",
    "from torch_geometric.utils import coalesce\n",
    "from typing import Callable, List, Optional\n",
    "\n",
    "\n",
    "class NodeClassificationInteractionDataset(InMemoryDataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        root: str,\n",
    "        node_file: str=None,\n",
    "        edge_file: str=None, \n",
    "        processed_file: str=None,\n",
    "        transform: Optional[Callable] = None,\n",
    "        pre_transform: Optional[Callable] = None,\n",
    "    ):\n",
    "        self.node_file = node_file\n",
    "        self.edge_file = edge_file\n",
    "        self.processed_file = processed_file\n",
    "        \n",
    "        super(NodeClassificationInteractionDataset, self).__init__(root, transform, pre_transform)\n",
    "\n",
    "        self.load(self.processed_paths[0])\n",
    "\n",
    "    @property\n",
    "    def raw_file_names(self):\n",
    "        return [self.node_file, self.edge_file]\n",
    "\n",
    "    @property\n",
    "    def processed_file_names(self):\n",
    "        return [self.processed_file]\n",
    "\n",
    "    def download(self):\n",
    "        pass\n",
    "\n",
    "    def process(self):\n",
    "        labels_path = os.path.join(self.raw_dir, self.node_file)\n",
    "        with open(labels_path, 'r') as f:\n",
    "            labels = [list(map(int, line.strip().split())) for line in f]\n",
    "\n",
    "        edgelist_path = os.path.join(self.raw_dir, self.edge_file)\n",
    "        with open(edgelist_path, 'r') as f:\n",
    "            edges = [\n",
    "                [int(start_node), int(end_node), float(weight)]\n",
    "                for line in f\n",
    "                for start_node, end_node, weight in [map(str, line.strip().split())]\n",
    "            ]\n",
    "\n",
    "        max_num_labels = max(len(node_labels) for node_labels in labels)\n",
    "        padded_labels = [node_labels + [0] * (max_num_labels - len(node_labels)) for node_labels in labels]\n",
    "        x = torch.tensor(padded_labels, dtype=torch.float32)\n",
    "\n",
    "        start_nodes = torch.tensor([edge[0] for edge in edges], dtype=torch.long)\n",
    "        end_nodes = torch.tensor([edge[1] for edge in edges], dtype=torch.long)\n",
    "        weights = torch.tensor([edge[2] for edge in edges], dtype=torch.float32)\n",
    "        edge_index = torch.stack([start_nodes, end_nodes], dim=0)\n",
    "        edge_attr = weights.view(-1, 1) \n",
    "        edge_index, edge_attr = coalesce(edge_index, edge_attr=edge_attr, num_nodes=x.size(0))\n",
    "\n",
    "        y = None\n",
    "\n",
    "        data = Data(\n",
    "            x=x, \n",
    "            edge_index=edge_index, \n",
    "            edge_attr=edge_attr,\n",
    "            y=y\n",
    "        )\n",
    "        \n",
    "        data = data if self.pre_transform is None else self.pre_transform(data)\n",
    "        self.save([data], self.processed_paths[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Node2Vec PPI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing...\n",
      "Done!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Data(x=[3890, 11], edge_index=[2, 76584], edge_attr=[76584, 1])"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Node2VecPPIDataset(NodeClassificationInteractionDataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        root: str,\n",
    "        transform: Optional[Callable] = None,\n",
    "        pre_transform: Optional[Callable] = None,\n",
    "    ):\n",
    "        super(Node2VecPPIDataset, self).__init__(\n",
    "            root, \n",
    "            'node2vec_PPI_labels.txt', 'node2vec_PPI.edgelist', 'node2vec_PPI_data.pt',\n",
    "            transform, pre_transform\n",
    "        )\n",
    "\n",
    "\n",
    "data_dir = '../data'\n",
    "dataset_name = 'node2vec_PPI'\n",
    "\n",
    "dataset = Node2VecPPIDataset(\n",
    "    osp.join(data_dir, dataset_name)\n",
    ")\n",
    "data = dataset[0]\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Mashup PPI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing...\n",
      "Done!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Data(x=[3831, 12], edge_index=[2, 596457], edge_attr=[596457, 1])"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MashupPPIDataset(NodeClassificationInteractionDataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        root: str,\n",
    "        transform: Optional[Callable] = None,\n",
    "        pre_transform: Optional[Callable] = None,\n",
    "    ):\n",
    "        super(MashupPPIDataset, self).__init__(\n",
    "            root, \n",
    "            'Mashup_PPI_labels.txt', 'Mashup_PPI.edgelist', 'Mashup_PPI_data.pt',\n",
    "            transform, pre_transform\n",
    "        )\n",
    "\n",
    "\n",
    "data_dir = '../data'\n",
    "dataset_name = 'Mashup_PPI'\n",
    "\n",
    "dataset = MashupPPIDataset(\n",
    "    osp.join(data_dir, dataset_name)\n",
    ")\n",
    "data = dataset[0]\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CoOccurences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing...\n",
      "Done!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Data(x=[25120, 6], edge_index=[2, 1658695], edge_attr=[1658695, 1])"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os.path as osp\n",
    "import torch\n",
    "from torch_geometric.data import InMemoryDataset, Data\n",
    "from torch_geometric.utils import coalesce\n",
    "from typing import Callable, List, Optional\n",
    "\n",
    "\n",
    "class CoocurenceDataset(InMemoryDataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        root: str,\n",
    "        transform: Optional[Callable] = None,\n",
    "        pre_transform: Optional[Callable] = None,\n",
    "    ):       \n",
    "        self.node_file = 'Clin_Term_COOC_labels.txt'\n",
    "        self.edge_file = 'Clin_Term_COOC.edgelist'\n",
    "        self.processed_file = 'Clin_Term_COOC_processed.pt'\n",
    "        super(InMemoryDataset, self).__init__(root, transform, pre_transform)\n",
    "\n",
    "        self.load(self.processed_paths[0])\n",
    "\n",
    "    @property\n",
    "    def raw_file_names(self):\n",
    "        return [self.node_file, self.edge_file]\n",
    "\n",
    "    @property\n",
    "    def processed_file_names(self):\n",
    "        return [self.processed_file]\n",
    "\n",
    "    def download(self):\n",
    "        pass\n",
    "\n",
    "    def process(self):\n",
    "        labels_path = os.path.join(self.raw_dir, self.node_file)\n",
    "        with open(labels_path, 'r') as f:\n",
    "            labels = [list(map(int, line.strip().split())) for line in f]\n",
    "\n",
    "        edgelist_path = os.path.join(self.raw_dir, self.edge_file)\n",
    "        with open(edgelist_path, 'r') as f:\n",
    "            edges = [\n",
    "                [int(start_node), int(end_node), float(weight)]\n",
    "                for line in f\n",
    "                for start_node, end_node, weight in [map(str, line.strip().split())]\n",
    "            ]\n",
    "\n",
    "        max_num_labels = max(len(node_labels) for node_labels in labels)\n",
    "        padded_labels = [node_labels + [0] * (max_num_labels - len(node_labels)) for node_labels in labels]\n",
    "        x = torch.tensor(padded_labels, dtype=torch.float32)\n",
    "\n",
    "        start_nodes = torch.tensor([edge[0] for edge in edges], dtype=torch.long)\n",
    "        end_nodes = torch.tensor([edge[1] for edge in edges], dtype=torch.long)\n",
    "        weights = torch.tensor([edge[2] for edge in edges], dtype=torch.float32)\n",
    "        edge_index = torch.stack([start_nodes, end_nodes], dim=0)\n",
    "        edge_attr = weights.view(-1, 1) \n",
    "        edge_index, edge_attr = coalesce(edge_index, edge_attr=edge_attr, num_nodes=x.size(0))\n",
    "\n",
    "        y = None\n",
    "\n",
    "        data = Data(\n",
    "            x=x, \n",
    "            edge_index=edge_index, \n",
    "            edge_attr=edge_attr,\n",
    "            y=y\n",
    "        )\n",
    "        \n",
    "        data = data if self.pre_transform is None else self.pre_transform(data)\n",
    "        self.save([data], self.processed_paths[0])\n",
    "\n",
    "\n",
    "data_dir = '../data'\n",
    "dataset_name = 'Clin_Term_COOC'\n",
    "\n",
    "dataset = CoocurenceDataset(\n",
    "    osp.join(data_dir, dataset_name)\n",
    ")\n",
    "data = dataset[0]\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([17378.,     0.,     0.,     0.,     0.,     0.])"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.x[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Link prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interaction Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path as osp\n",
    "import torch\n",
    "from torch_geometric.data import InMemoryDataset, Data\n",
    "from torch_geometric.utils import coalesce\n",
    "from typing import Callable, List, Optional\n",
    "\n",
    "\n",
    "class LinkPredictionInteractionDataset(InMemoryDataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        root: str,\n",
    "        node_file: str=None,\n",
    "        edge_file: str=None, \n",
    "        processed_file: str=None,\n",
    "        transform: Optional[Callable] = None,\n",
    "        pre_transform: Optional[Callable] = None,\n",
    "    ):\n",
    "        self.node_file = node_file\n",
    "        self.edge_file = edge_file\n",
    "        self.processed_file = processed_file\n",
    "        \n",
    "        super(LinkPredictionInteractionDataset, self).__init__(root, transform, pre_transform)\n",
    "\n",
    "        self.load(self.processed_paths[0])\n",
    "\n",
    "    @property\n",
    "    def raw_file_names(self):\n",
    "        return [self.node_file, self.edge_file]\n",
    "\n",
    "    @property\n",
    "    def processed_file_names(self):\n",
    "        return [self.processed_file]\n",
    "\n",
    "    def process(self):\n",
    "        labels_path = os.path.join(self.raw_dir, self.node_file)\n",
    "        with open(labels_path, 'r') as f:\n",
    "            next(f)\n",
    "            labels = [\n",
    "                int(node)\n",
    "                for line in f\n",
    "                for node, _ in [map(str, line.strip().split())]\n",
    "            ]\n",
    "\n",
    "        edgelist_path = os.path.join(self.raw_dir, self.edge_file)\n",
    "        with open(edgelist_path, 'r') as f:\n",
    "            edges = [\n",
    "                [int(start_node), int(end_node)]\n",
    "                for line in f\n",
    "                for start_node, end_node in [map(str, line.strip().split())]\n",
    "            ]\n",
    "\n",
    "        x = torch.tensor(labels, dtype=torch.int32).view(-1, 1)\n",
    "\n",
    "        start_nodes = torch.tensor([edge[0] for edge in edges], dtype=torch.long)\n",
    "        end_nodes = torch.tensor([edge[1] for edge in edges], dtype=torch.long)\n",
    "        edge_index = torch.stack([start_nodes, end_nodes], dim=0)\n",
    "        edge_index = coalesce(edge_index, num_nodes=x.size(0))\n",
    "\n",
    "        y = None\n",
    "\n",
    "        data = Data(\n",
    "            x=x, \n",
    "            edge_index=edge_index, \n",
    "            y=y\n",
    "        )\n",
    "        \n",
    "        data = data if self.pre_transform is None else self.pre_transform(data)\n",
    "        self.save([data], self.processed_paths[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### String PPI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(x=[15131, 1], edge_index=[2, 359776])"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class StringPPIDataset(LinkPredictionInteractionDataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        root: str,\n",
    "        transform: Optional[Callable] = None,\n",
    "        pre_transform: Optional[Callable] = None,\n",
    "    ):\n",
    "        super(StringPPIDataset, self).__init__(\n",
    "            root, \n",
    "            'node_list.txt', 'STRING_PPI.edgelist', 'string_PPI_data.pt',\n",
    "            transform, pre_transform\n",
    "        )\n",
    "  \n",
    "data_dir = '../data'\n",
    "dataset_name = 'STRING_PPI'\n",
    "\n",
    "dataset = StringPPIDataset(\n",
    "    osp.join(data_dir, dataset_name)\n",
    ")\n",
    "data = dataset[0]\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### DrugBank DDI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(x=[2191, 1], edge_index=[2, 242027])"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class DrugBankDDIDataset(LinkPredictionInteractionDataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        root: str,\n",
    "        transform: Optional[Callable] = None,\n",
    "        pre_transform: Optional[Callable] = None,\n",
    "    ):\n",
    "        super(DrugBankDDIDataset, self).__init__(\n",
    "            root, \n",
    "            'node_list.txt', 'DrugBank_DDI.edgelist', 'drugbank_DDI_data.pt',\n",
    "            transform, pre_transform\n",
    "        )\n",
    "\n",
    "data_dir = '../data'\n",
    "dataset_name = 'DrugBank_DDI'\n",
    "\n",
    "dataset = DrugBankDDIDataset(\n",
    "    osp.join(data_dir, dataset_name)\n",
    ")\n",
    "data = dataset[0]\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Association Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13545\n",
      "13545\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "data_dir = '../data'\n",
    "dataset_name = 'NDFRT_DDA'\n",
    "\n",
    "labels_path = os.path.join(data_dir, dataset_name, 'node_list.txt')\n",
    "with open(labels_path, 'r') as f:\n",
    "    next(f)\n",
    "    labels = [\n",
    "        [node_id, UMLS_CUI, agent_type]\n",
    "        for line in f\n",
    "        for node_id, UMLS_CUI, agent_type in [map(str, line.strip().split())]\n",
    "    ]\n",
    "\n",
    "node_id_list = [label[0] for label in labels]\n",
    "UMLS_CUI_list = [label[1] for label in labels]\n",
    "agent_type_list = [label[2] for label in labels]\n",
    "\n",
    "print(len(np.unique(node_id_list)))\n",
    "print(len(np.unique(UMLS_CUI_list)))\n",
    "print(len(np.unique(agent_type_list)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12765\n",
      "12765\n",
      "12765\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "data_dir = '../data'\n",
    "dataset_name = 'CTD_DDA'\n",
    "\n",
    "labels_path = os.path.join(data_dir, dataset_name, 'node_list.txt')\n",
    "with open(labels_path, 'r') as f:\n",
    "    next(f)\n",
    "    labels = [\n",
    "        [node_id, CTD_id, UMLS_CUI, agent_type]\n",
    "        for line in f\n",
    "        for node_id, CTD_id, UMLS_CUI, agent_type in [map(str, line.strip().split())]\n",
    "    ]\n",
    "\n",
    "node_id_list = [label[0] for label in labels]\n",
    "CTD_id_list = [label[1] for label in labels]\n",
    "UMLS_CUI_list = [label[2] for label in labels]\n",
    "agent_type_list = [label[3] for label in labels]\n",
    "\n",
    "\n",
    "print(len(np.unique(node_id_list)))\n",
    "print(len(np.unique(CTD_id_list)))\n",
    "print(len(np.unique(UMLS_CUI_list)))\n",
    "print(len(np.unique(agent_type_list)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.24951037994516256"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dir = '../data'\n",
    "dataset_name = 'CTD_DDA'\n",
    "\n",
    "labels_path = os.path.join(data_dir, dataset_name, 'node_list.txt')\n",
    "with open(labels_path, 'r') as f:\n",
    "    next(f)\n",
    "    labels = [\n",
    "        list(map(str, line.strip().split()))\n",
    "        for line in f        \n",
    "    ]\n",
    "labels = [1 if label[-1] == 'disease' else 0 for label in labels]\n",
    "\n",
    "sum(labels)/len(labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path as osp\n",
    "import torch\n",
    "from torch_geometric.data import InMemoryDataset, Data\n",
    "from torch_geometric.utils import coalesce\n",
    "from typing import Callable, List, Optional\n",
    "\n",
    "\n",
    "class LinkPredictionAssociationDataset(InMemoryDataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        root: str,\n",
    "        node_file: str=None,\n",
    "        edge_file: str=None, \n",
    "        processed_file: str=None,\n",
    "        transform: Optional[Callable] = None,\n",
    "        pre_transform: Optional[Callable] = None,\n",
    "    ):\n",
    "        self.node_file = node_file\n",
    "        self.edge_file = edge_file\n",
    "        self.processed_file = processed_file\n",
    "        \n",
    "        super(LinkPredictionAssociationDataset, self).__init__(root, transform, pre_transform)\n",
    "\n",
    "        self.load(self.processed_paths[0])\n",
    "\n",
    "    @property\n",
    "    def raw_file_names(self):\n",
    "        return [self.node_file, self.edge_file]\n",
    "\n",
    "    @property\n",
    "    def processed_file_names(self):\n",
    "        return [self.processed_file]\n",
    "\n",
    "    def process(self):\n",
    "        labels_path = os.path.join(self.raw_dir, self.node_file)\n",
    "        with open(labels_path, 'r') as f:\n",
    "            next(f)\n",
    "            labels = [\n",
    "                list(map(str, line.strip().split()))\n",
    "                for line in f        \n",
    "            ]\n",
    "        labels = [1 if label[-1] == 'disease' else 0 for label in labels]\n",
    "\n",
    "        edgelist_path = os.path.join(self.raw_dir, self.edge_file)\n",
    "        with open(edgelist_path, 'r') as f:\n",
    "            edges = [\n",
    "                [int(start_node), int(end_node)]\n",
    "                for line in f\n",
    "                for start_node, end_node in [map(str, line.strip().split())]\n",
    "            ]\n",
    "\n",
    "        x = torch.tensor(labels, dtype=torch.int32).view(-1, 1)\n",
    "\n",
    "        start_nodes = torch.tensor([edge[0] for edge in edges], dtype=torch.long)\n",
    "        end_nodes = torch.tensor([edge[1] for edge in edges], dtype=torch.long)\n",
    "        edge_index = torch.stack([start_nodes, end_nodes], dim=0)\n",
    "        edge_index = coalesce(edge_index, num_nodes=x.size(0))\n",
    "\n",
    "        y = None\n",
    "\n",
    "        data = Data(\n",
    "            x=x, \n",
    "            edge_index=edge_index, \n",
    "            y=y\n",
    "        )\n",
    "        \n",
    "        data = data if self.pre_transform is None else self.pre_transform(data)\n",
    "        self.save([data], self.processed_paths[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### CTD DDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing...\n",
      "Done!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Data(x=[12765, 1], edge_index=[2, 92813])"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class CTDDDADataset(LinkPredictionAssociationDataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        root: str,\n",
    "        transform: Optional[Callable] = None,\n",
    "        pre_transform: Optional[Callable] = None,\n",
    "    ):\n",
    "        super(CTDDDADataset, self).__init__(\n",
    "            root, \n",
    "            'node_list.txt', 'CTD_DDA.edgelist', 'CTD_DDA.pt',\n",
    "            transform, pre_transform\n",
    "        )\n",
    "\n",
    "data_dir = '../data'\n",
    "dataset_name = 'CTD_DDA'\n",
    "\n",
    "dataset = CTDDDADataset(\n",
    "    osp.join(data_dir, dataset_name)\n",
    ")\n",
    "data = dataset[0]\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### NDFRT DDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing...\n",
      "Done!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Data(x=[13545, 1], edge_index=[2, 56515])"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class NDFRTDDADataset(LinkPredictionAssociationDataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        root: str,\n",
    "        transform: Optional[Callable] = None,\n",
    "        pre_transform: Optional[Callable] = None,\n",
    "    ):\n",
    "        super(NDFRTDDADataset, self).__init__(\n",
    "            root, \n",
    "            'node_list.txt', 'NDFRT_DDA.edgelist', 'NDFRT_DDA.pt',\n",
    "            transform, pre_transform\n",
    "        )\n",
    "\n",
    "data_dir = '../data'\n",
    "dataset_name = 'NDFRT_DDA'\n",
    "\n",
    "dataset = NDFRTDDADataset(\n",
    "    osp.join(data_dir, dataset_name)\n",
    ")\n",
    "data = dataset[0]\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'num_samples' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 56\u001b[0m\n\u001b[0;32m     52\u001b[0m data_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../data\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     53\u001b[0m dataset_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnode2vec_PPI\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m---> 56\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43mNode2VecPPI\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     57\u001b[0m \u001b[43m    \u001b[49m\u001b[43mosp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     58\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[13], line 16\u001b[0m, in \u001b[0;36mNode2VecPPI.__init__\u001b[1;34m(self, root, transform, pre_transform)\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m     12\u001b[0m     root: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m     13\u001b[0m     transform: Optional[Callable] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m     14\u001b[0m     pre_transform: Optional[Callable] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m     15\u001b[0m ):\n\u001b[1;32m---> 16\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mInMemoryDataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mroot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpre_transform\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_processed_data():\n\u001b[0;32m     19\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocess()\n",
      "File \u001b[1;32mc:\\Users\\alexa\\anaconda3\\envs\\gcl-py311-cu121\\Lib\\site-packages\\torch_geometric\\data\\dataset.py:118\u001b[0m, in \u001b[0;36mDataset.__init__\u001b[1;34m(self, root, transform, pre_transform, pre_filter, log, force_reload)\u001b[0m\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_download()\n\u001b[0;32m    117\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhas_process:\n\u001b[1;32m--> 118\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_process\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\alexa\\anaconda3\\envs\\gcl-py311-cu121\\Lib\\site-packages\\torch_geometric\\data\\dataset.py:263\u001b[0m, in \u001b[0;36mDataset._process\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    260\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mProcessing...\u001b[39m\u001b[38;5;124m'\u001b[39m, file\u001b[38;5;241m=\u001b[39msys\u001b[38;5;241m.\u001b[39mstderr)\n\u001b[0;32m    262\u001b[0m fs\u001b[38;5;241m.\u001b[39mmakedirs(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocessed_dir, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m--> 263\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    265\u001b[0m path \u001b[38;5;241m=\u001b[39m osp\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocessed_dir, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpre_transform.pt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    266\u001b[0m fs\u001b[38;5;241m.\u001b[39mtorch_save(_repr(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpre_transform), path)\n",
      "Cell \u001b[1;32mIn[13], line 37\u001b[0m, in \u001b[0;36mNode2VecPPI.process\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     33\u001b[0m data_list \u001b[38;5;241m=\u001b[39m []  \u001b[38;5;66;03m# List to store Data objects\u001b[39;00m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;66;03m# Your data processing logic here\u001b[39;00m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;66;03m# Construct Data objects and append to data_list\u001b[39;00m\n\u001b[1;32m---> 37\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[43mnum_samples\u001b[49m):\n\u001b[0;32m     38\u001b[0m     \u001b[38;5;66;03m# Example: Construct a Data object\u001b[39;00m\n\u001b[0;32m     39\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m  \u001b[38;5;66;03m# Node features\u001b[39;00m\n\u001b[0;32m     40\u001b[0m     edge_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m  \u001b[38;5;66;03m# Edge indices\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'num_samples' is not defined"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "from torch_geometric.data import (\n",
    "    Data,\n",
    "    InMemoryDataset\n",
    ")\n",
    "\n",
    "\n",
    "class Node2VecPPI(InMemoryDataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        root: str,\n",
    "        transform: Optional[Callable] = None,\n",
    "        pre_transform: Optional[Callable] = None,\n",
    "    ):\n",
    "        super(InMemoryDataset, self).__init__(root, transform, pre_transform)\n",
    "        self.data, self.slices = self.load(self.processed_paths[0])\n",
    "    \n",
    "    @property\n",
    "    def raw_file_names(self) -> List[str]:\n",
    "        return ['node2vec_PPI_labels.txt', 'node2vec_PPI.edgelist']\n",
    "    \n",
    "    @property\n",
    "    def processed_file_names(self):\n",
    "        # Return a list of processed file names if applicable\n",
    "        return ['node2vec_PPI.pt']\n",
    "    \n",
    "\n",
    "    def process(self):\n",
    "        data_list = []  # List to store Data objects\n",
    "        \n",
    "        # Your data processing logic here\n",
    "        # Construct Data objects and append to data_list\n",
    "        for i in range(num_samples):\n",
    "            # Example: Construct a Data object\n",
    "            x = ...  # Node features\n",
    "            edge_index = ...  # Edge indices\n",
    "            y = ...  # Target label\n",
    "            \n",
    "            data = Data(x=x, edge_index=edge_index, y=y)\n",
    "            data_list.append(data)\n",
    "        \n",
    "        # max_num_labels = max(len(node_labels) for node_labels in labels)\n",
    "        # padded_labels = [node_labels + [0] * (max_num_labels - len(node_labels)) for node_labels in labels]\n",
    "        # x = torch.tensor(padded_labels, dtype=torch.float32) # Node features (assuming unbalanced lists)\n",
    "        # x = torch.tensor(labels, dtype=torch.float32).view(-1, 1)  # Node features (assuming one-dimensional labels)\n",
    "        # x = torch.tensor(labels, dtype=torch.float32)  # Node features (assuming labels are integers)\n",
    "\n",
    "\n",
    "\n",
    "            \n",
    "        data, slices = self.collate(data_list)\n",
    "        torch.save((data, slices), self.processed_paths[0])\n",
    "\n",
    "\n",
    "import os.path as osp\n",
    "\n",
    "data_dir = '../data'\n",
    "dataset_name = 'node2vec_PPI'\n",
    "\n",
    "\n",
    "dataset = Node2VecPPI(\n",
    "    osp.join(data_dir, dataset_name)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(x=[3890, 11], edge_index=[2, 76584], edge_attr=[76584, 1])"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "graph-contrastive-learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
